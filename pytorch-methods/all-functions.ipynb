{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Okay, I understand. You want a comprehensive list of PyTorch methods and built-in functions that are most frequently used (99% of the time) in real-world projects. This is a tall order, as PyTorch is vast! However, I'll give you the most essential ones categorized for better understanding. I'll cover as much as possible within the response length limit.\n\n**Note:** It's impossible to list *every* single method, but I'll focus on the ones that form the core of most PyTorch workflows.\n\n**1. Tensor Creation and Manipulation**\n\n*   **`torch.tensor(data, dtype=None, device=None, requires_grad=False)`:** Creates a tensor from data (lists, NumPy arrays).\n*   **`torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`:** Creates a tensor filled with zeros.\n*   **`torch.ones(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`:** Creates a tensor filled with ones.\n*   **`torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`:** Creates a 1D tensor with a range of values.\n*   **`torch.linspace(start, end, steps, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`:** Creates a 1D tensor with evenly spaced values.\n*   **`torch.rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`:** Creates a tensor with random values from a uniform distribution.\n*   **`torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`:** Creates a tensor with random values from a standard normal distribution.\n*   **`torch.randint(low=0, high, size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`:** Creates a tensor with random integers.\n*   **`tensor.view(*shape)`:**  Reshapes a tensor without changing its data.\n*   **`tensor.reshape(*shape)`:** Reshapes a tensor, potentially copying data if needed.\n*   **`tensor.squeeze(dim=None)`:** Removes dimensions of size 1.\n*   **`tensor.unsqueeze(dim)`:** Adds a dimension of size 1 at a specified position.\n*   **`tensor.transpose(dim0, dim1)`:** Swaps two dimensions.\n*   **`tensor.permute(*dims)`:** Rearranges dimensions.\n*   **`torch.cat(tensors, dim=0, out=None)`:** Concatenates tensors along a given dimension.\n*   **`torch.stack(tensors, dim=0, out=None)`:**  Concatenates tensors along a new dimension.\n*   **`tensor.to(device=None, dtype=None, non_blocking=False, copy=False, memory_format=torch.preserve_format)`:** Moves and/or casts a tensor to a specific device and/or data type.\n\n**2. Tensor Operations (Mathematical)**\n\n*   **`torch.add(input, other, out=None)` or `input + other`:** Element-wise addition.\n*   **`torch.sub(input, other, out=None)` or `input - other`:** Element-wise subtraction.\n*   **`torch.mul(input, other, out=None)` or `input * other`:** Element-wise multiplication.\n*   **`torch.div(input, other, out=None)` or `input / other`:** Element-wise division.\n*   **`torch.matmul(input, other, out=None)`:** Matrix multiplication.\n*   **`torch.mm(input, other, out=None)`:** Matrix multiplication of 2D matrices (no broadcasting).\n*   **`torch.bmm(input, other, out=None)`:** Batch matrix multiplication.\n*   **`torch.dot(input, other, out=None)`:** Dot product of two 1D tensors.\n*   **`torch.sum(input, dim=None, keepdim=False, dtype=None)`:** Sum of tensor elements (optionally along a specific dimension).\n*   **`torch.mean(input, dim=None, keepdim=False, dtype=None)`:** Mean of tensor elements.\n*   **`torch.std(input, dim=None, unbiased=True, keepdim=False)`:** Standard deviation.\n*   **`torch.max(input, dim=None, keepdim=False)`:** Maximum value (and optionally its index).\n*   **`torch.min(input, dim=None, keepdim=False)`:** Minimum value.\n*   **`torch.argmax(input, dim=None, keepdim=False)`:** Index of the maximum value.\n*   **`torch.argmin(input, dim=None, keepdim=False)`:** Index of the minimum value.\n*   **`torch.exp(input, out=None)`:** Element-wise exponential.\n*   **`torch.log(input, out=None)`:** Element-wise natural logarithm.\n*   **`torch.sqrt(input, out=None)`:** Element-wise square root.\n*   **`torch.pow(input, exponent, out=None)`:** Element-wise power.\n*   **`torch.abs(input, out=None)`:** Element-wise absolute value.\n*   **`torch.sin(input, out=None)`, `torch.cos()`, `torch.tan()`:** Trigonometric functions.\n\n**3. Neural Network Modules (`torch.nn`)**\n\n*   **`nn.Module`:** Base class for all neural network modules.\n*   **`nn.Linear(in_features, out_features, bias=True)`:** Fully connected (linear) layer.\n*   **`nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`:** 2D convolutional layer.\n*   **`nn.Conv1d()`, `nn.Conv3d()`:** 1D and 3D convolutional layers.\n*   **`nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)`:** 2D max pooling.\n*   **`nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)`:** 2D average pooling.\n*   **`nn.AdaptiveAvgPool2d()`:** Adaptive average pooling.\n*   **`nn.ReLU(inplace=False)`:** Rectified Linear Unit activation.\n*   **`nn.Sigmoid()`:** Sigmoid activation.\n*   **`nn.Tanh()`:** Hyperbolic tangent activation.\n*   **`nn.LeakyReLU(negative_slope=0.01, inplace=False)`:** Leaky ReLU.\n*   **`nn.Softmax(dim=None)`:** Softmax function.\n*   **`nn.Dropout(p=0.5, inplace=False)`:** Dropout layer.\n*   **`nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)`:** Batch Normalization for 2D data.\n*   **`nn.Embedding(num_embeddings, embedding_dim)`:** Embedding layer (for words, etc.).\n*   **`nn.LSTM()`, `nn.GRU()`:** Recurrent layers (LSTM and GRU).\n*   **`nn.Sequential(*args)`:** Container to stack modules sequentially.\n*   **`nn.CrossEntropyLoss()`, `nn.MSELoss()`, `nn.BCELoss()`, `nn.L1Loss()`:** Common loss functions.\n*   `nn.BCEWithLogitsLoss()`\n\n**4. Optimizers (`torch.optim`)**\n\n*   **`optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)`:** Stochastic Gradient Descent.\n*   **`optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)`:** Adam optimizer.\n*   **`optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)`:** RMSprop optimizer.\n*   **`optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)`:** Adagrad optimizer.\n*   **`optimizer.step()`:** Updates model parameters based on gradients.\n*   **`optimizer.zero_grad()`:** Sets gradients of optimized tensors to zero.\n\n**5. Data Loading (`torch.utils.data`)**\n\n*   **`data.Dataset`:** Abstract class for representing a dataset.\n*   **`data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None)`:** Loads data in batches, shuffles, and handles multi-processing.\n*   **`data.TensorDataset(*tensors)`:** Wraps tensors into a dataset.\n*   **`data.random_split(dataset, lengths)`:** Splits a dataset into random subsets.\n\n**6. Automatic Differentiation (`torch.autograd`)**\n\n*   **`tensor.backward(gradient=None, retain_graph=None, create_graph=False)`:** Computes gradients of a tensor w.r.t. graph leaves.\n*   **`tensor.grad`:**  Accesses the gradient of a tensor.\n*   **`tensor.requires_grad`:** Attribute to indicate if a tensor requires gradient computation.\n*   **`torch.no_grad()`:** Context manager to disable gradient calculation (for inference).\n*   **`torch.enable_grad()`:** Context manager to enable gradient calculation.\n*   **`torch.set_grad_enabled(mode)`:**  Sets whether gradient computation is enabled or disabled globally.\n\n**7. Saving and Loading Models**\n\n*   **`torch.save(obj, f, pickle_module=pickle, pickle_protocol=2, _use_new_zipfile_serialization=True)`:** Saves a model's state dictionary, an entire model, or other objects.\n*   **`torch.load(f, map_location=None, pickle_module=pickle, **pickle_load_args)`:** Loads a saved object.\n*   **`model.state_dict()`:** Returns a dictionary containing a whole state of the module.\n*   **`model.load_state_dict(state_dict, strict=True)`:** Copies parameters and buffers from `state_dict` into the module and its descendants.\n\n**8. Distributed Training (`torch.distributed`)**\n\n*   **`torch.distributed.init_process_group(backend, init_method=None, timeout=datetime.timedelta(seconds=1800), world_size=-1, rank=-1, store=None, group_name='')`:** Initializes a process group for distributed training.\n*   **`torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0, broadcast_buffers=True, process_group=None, bucket_cap_mb=25, find_unused_parameters=False, check_reduction=False, gradient_as_bucket_view=False)`:** Wraps a module for data parallelism.\n\n**9. Utilities**\n\n*   **`torch.cuda.is_available()`:** Checks if a CUDA-enabled GPU is available.\n*   **`torch.cuda.device_count()`:** Returns the number of CUDA devices available.\n*   **`torch.from_numpy(ndarray)`:** Creates a tensor from a NumPy array (shares memory).\n*   **`tensor.numpy()`:** Converts a tensor to a NumPy array (shares memory if possible).\n\n**10. Torchvision**\n\n*   `torchvision.transforms`: Contains common image transformations for data augmentation.\n    *   `transforms.Compose`\n    *   `transforms.ToTensor`\n    *   `transforms.Normalize`\n    *   `transforms.RandomHorizontalFlip`\n    *   `transforms.RandomResizedCrop`\n    *   `transforms.Resize`\n*   `torchvision.datasets`: Contains many common computer vision datasets.\n    *   `datasets.ImageFolder`\n    *   `datasets.CIFAR10`\n    *   `datasets.MNIST`\n\n**11. Other Important Modules**\n\n*   `torch.nn.functional`: Functional API for many operations (often used for activations, pooling in custom modules).\n    *   `F.relu`\n    *   `F.softmax`\n    *   `F.max_pool2d`\n    *   `F.cross_entropy`\n*   `torch.jit`: For creating TorchScript models (used for optimization and deployment).\n    *   `torch.jit.trace`\n    *   `torch.jit.script`\n\nThis is a long list, and it still doesn't cover everything! But, these are the PyTorch functions and methods you'll likely encounter and use most frequently in your deep learning projects. Remember to refer to the official PyTorch documentation for in-depth details and more advanced features as you need them. If the response length permits, I'll add more!\n","metadata":{}}]}